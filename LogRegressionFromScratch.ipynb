{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zVfYjanWGWq"
   },
   "source": [
    "Regresja logistyczna napisana od zera, tylko przy użyciu pakietu numpy.\n",
    "Algorytm SGD z momentum i regularyzacją.\n",
    "\n",
    "Zbiór MNIST dostępny jest pod linkami: \n",
    "\n",
    "(zbiór treningowy):\n",
    " - http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "(zbiór walidacyjny):\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Htj5iNRhWgz6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    \n",
    "  def tune(self, weights, alpha=0.05, beta = 0.98, lamb = 0.9) -> None:\n",
    "    '''Wagi modelu to wektor przechowujący wszystkie współczynniki odnoszące się do każdego \n",
    "    piksela plus stałą.\n",
    "    '''\n",
    "    self.weights = weights \n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.lamb = lamb\n",
    "    self.primes = [2,3,5,7]\n",
    "    \n",
    "  @staticmethod\n",
    "  def _sigmoid(s):\n",
    "    '''Metoda odwzorowująca sigmoid'''\n",
    "    s = np.exp(-s)\n",
    "    return 1.0 / (1.0 + s)\n",
    "\n",
    "  def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "    '''Metoda zwracająca przewidywanie modelu'''\n",
    "    x_vec = np.array(np.insert(X, 0, 1))\n",
    "    return self._sigmoid(np.dot(self.weights, x_vec))\n",
    "\n",
    "  def _cost_per_point(self, X: np.ndarray, y: np.ndarray):\n",
    "    '''Metoda zwracająca koszt regresji liniowej dla pojedynczego punktu'''\n",
    "    y_pred = self.predict(X)\n",
    "    # Sprawdzane jest, czy etykietka należy do cyfr pierwszych, jeżeli tak\n",
    "    # prawdopodobieństwo 1\n",
    "    y_true = 1.0 if y in self.primes else 0.0\n",
    "    cost_per_point = y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1 - y_pred)\n",
    "    return round(cost_per_point)\n",
    "\n",
    "  def _total_cost(self, X: np.ndarray, y: np.ndarray):\n",
    "    '''Metoda sumująca koszt regresji zbioru'''\n",
    "    cost = 0.0\n",
    "    # Dodawany jest koszt dla każdego punktu\n",
    "    for img, label in zip(X, y):\n",
    "        cost+= self._cost_per_point(img, label)\n",
    "    # Dodatkowo jako część regularyzacji dodawany jest kwadrat wag\n",
    "    cost -= (self.lamb*np.dot(np.transpose(self.weights), self.weights))/(2*self.weights.size)\n",
    "    return cost\n",
    "\n",
    "  def _shuffle_datasets(self, images: np.ndarray, labels: np.ndarray, proportion = 0.8, random_seed:int = -1):\n",
    "    '''Metoda do ponownego przetasowania danych i podzielenia ich na zbiory - w tym wypadku treningowy \n",
    "    i walidacyjny. Do trenowania modelu postanowiłam wykorzystać metodę walidacji krzyżowej.\n",
    "    '''\n",
    "    if(random_seed<0):\n",
    "        np.random.seed()\n",
    "    else:\n",
    "        np.random.seed(random_seed)\n",
    "    # Aby nie pomieszać obrazków i etykietek, najpierw przetasowuję indeksy\n",
    "    total = np.arange(labels.size,dtype=np.int64)\n",
    "    np.random.shuffle(total)\n",
    "    # Puste tablice na przetasowane etykietki i obrazki\n",
    "    img = np.empty(images.size,dtype=np.object)\n",
    "    lab = np.empty(images.size,dtype=np.int64)\n",
    "    for i in range(0, labels.size):\n",
    "        # Do tablic na obrazki i etykietki trafiają obiekty o nowym indeksie\n",
    "        img[i] = images[total[i]]\n",
    "        lab[i] = labels[total[i]]\n",
    "    # Podział obrazków i etykietek na treningowe i testowe\n",
    "    split = [round(0.8*total.size)]\n",
    "    img = np.array_split(img, split, axis = 0)\n",
    "    lab = np.array_split(lab, split)\n",
    "    return img[0], img[1], lab[0], lab[1]\n",
    "    \n",
    "  def fit(self, X: np.ndarray, y: np.ndarray, epochs:int = 3, after:int = 3, verbose:bool = True) -> None:\n",
    "    '''Metoda odpowiadająca za trenowanie modelu. Podczas przechodzenia w losowej \n",
    "    kolejności przez elementy zbioru treningowego szacowany jest koszt przejścia \n",
    "    przez zbiór treningowy (do późniejszego porównania z poprzednią iteracją i\n",
    "    przerwania, gdyby koszt zaczął rosnąć). W podobnych celach wykorzystywany jest\n",
    "    koszt na zbiorze walidacyjnym, obliczany co pełne przejście przez zbiór.\n",
    "    '''\n",
    "    previous_compute_cost = self._total_cost(X, y)\n",
    "    best_weights = self.weights\n",
    "    # Ze względu na losową naturę SGD, algorytm powtarzany jest epochs razy.\n",
    "    # Na końcu zapisywane są wagi, które pozwoliły na otrzymanie najmniejszego\n",
    "    # całkowitego kosztu.\n",
    "    \n",
    "    for e in range(0, epochs):\n",
    "        # przetasowanie zbioru, podział na testowy i walidacyjny\n",
    "        train, val, label_train, label_val =\\\n",
    "        self._shuffle_datasets(X, y, random_seed = int(abs(previous_compute_cost)))\n",
    "        random_inputs = list(zip(train, label_train))\n",
    "        # obliczenie całkowitego kosztu na zbiorze walidacyjnym\n",
    "        # i treningowym, dla porównania później\n",
    "        previous_val_loss = self._total_cost(val, label_val)\n",
    "        previous_train_cost = self._total_cost(train, label_train)\n",
    "        \n",
    "        cost_index = 0\n",
    "        val_index = 0\n",
    "        momentum = 0\n",
    "            \n",
    "        weights_of_min = self.weights\n",
    "        loss_of_min = previous_val_loss\n",
    "            \n",
    "        # iteracja SGD po zbiorze następuje tak długo, aż przez ilość\n",
    "        # iteracji ustalonych w after koszt treningowy nie będzie maleć,\n",
    "        # albo koszt na zbiorze walidacyjnym będzie większy od najmniejszego\n",
    "        # (oznacza to, ze osiągnęliśmy minimum i cza przerwać funkcję)\n",
    "        while(cost_index<after and val_index <after):\n",
    "            train_cost = 0\n",
    "                \n",
    "            for i in range(train.size):\n",
    "                x_vec = np.array(np.insert(random_inputs[i][0], 0, 1))\n",
    "                y_pred = self.predict(random_inputs[i][0])\n",
    "                y_true = 1.0 if random_inputs[i][1] in self.primes else 0.0\n",
    "                # obliczany jest gradient dla każdego punktu\n",
    "                gradient = (y_pred - y_true) * x_vec\n",
    "                # obliczane jest momentum dla kolejnego punktu\n",
    "                momentum = self.beta * momentum + (1-self.beta)* gradient\n",
    "                # dodawana jest kolejna wartość do kosztu treningowego\n",
    "                train_cost += self._cost_per_point(random_inputs[i][0], random_inputs[i][1])\n",
    "                # wagi aktualizowane są zgodnie z obliczonym gradientem i momentum\n",
    "                # oraz dodaną regularyzacją\n",
    "                self.weights -= self.alpha * (momentum + self.lamb*self.weights/self.weights.size)\n",
    "                \n",
    "            # obliczanie kosztu zbioru walidacji\n",
    "            val_loss = self._total_cost(val, label_val)\n",
    "            # koszt zbioru treningowego aktualizowany jest o regularyzację\n",
    "            train_cost -= (self.lamb*np.dot(np.transpose(self.weights), self.weights))/(2*self.weights.size)\n",
    "             \n",
    "            # porównanie kosztu treningowego obecnego i poprzedniego\n",
    "            # ponieważ koszt zawsze jest ujemny, szukany jest koszt o mniejszej wartości\n",
    "            # bezwzględnej\n",
    "            previous_train_cost = np.around(previous_train_cost, decimals = 6)\n",
    "            train_cost = np.around(train_cost, decimals = 6)\n",
    "            \n",
    "            cost_index = cost_index + 1 if(previous_train_cost>=train_cost) else 0\n",
    "                  \n",
    "            # podobnie koszt zbioru walidacyjnego     \n",
    "            if(np.around(loss_of_min, decimals = 6)>= np.around(val_loss, decimals = 6)):\n",
    "                val_index += 1\n",
    "            else:\n",
    "                # przechowywane są wagi, dla których koszt \n",
    "                # zbioru walidacyjnego był najkorzystniejszy\n",
    "                val_index = 0\n",
    "                weights_of_min = self.weights\n",
    "                loss_of_min = val_loss\n",
    "                    \n",
    "            previous_train_cost = train_cost\n",
    "            previous_val_loss = val_loss\n",
    "            if(verbose):\n",
    "                print(f'epoch: {e}')\n",
    "                print(f'training cost: {train_cost}')\n",
    "                print(f'validation loss: {val_loss}')\n",
    "                print(f'training index: {cost_index}')\n",
    "                print(f'validation index: {val_index}')\n",
    "                print('-----------------------------------')\n",
    "                \n",
    "        self.weights = weights_of_min\n",
    "        compute_cost = self._total_cost(X, y)\n",
    "        # jeżeli wagi osiągnięte w tej epoce zwracają lepszy koszt od poprzednich\n",
    "        # zapisz je jako najlepsze\n",
    "        if(previous_compute_cost < compute_cost):\n",
    "            best_params = self.weights\n",
    "        previous_compute_cost = compute_cost\n",
    "        self.weights = np.zeros(len(train[0]) + 1)\n",
    "        \n",
    "    # Uaktualnij wagi do najlepszych\n",
    "    self.weights = best_weights\n",
    "\n",
    "  @staticmethod\n",
    "  def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # Oblicz skuteczność\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        total += 1   \n",
    "        if true >= 0.5 and pred == 1:\n",
    "            # Właściwa klasyfikacji cyfry pierwszej\n",
    "            correct += 1\n",
    "                \n",
    "        if true < 0.5 and pred != 1:\n",
    "            # Właściwa klasyfikacja cyfry złożonej\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pliki z danymi zostały uprzednio rozpakowane w tym samym katalogu, w którym znajduje się notebook'''\n",
    "def read_labels(filepath: str) -> np.ndarray:\n",
    "    '''Metoda odczytująca etykiety obrazków z pliku zgodnie ze specyfikacją ze strony MNIST'''\n",
    "    try:\n",
    "        f = open(filepath, 'rb')\n",
    "        # pierwsze cztery bajty to magiczna liczba\n",
    "        f.read(4)\n",
    "        if f.mode == 'rb':\n",
    "            # kolejne cztery zawierają informację o ilości etykiet\n",
    "            buf = f.read(4)\n",
    "            number_of_labels = int.from_bytes(buf,'big')\n",
    "            result = np.empty(number_of_labels)\n",
    "            for i in range(0,number_of_labels):\n",
    "                # odczytywanie etykiet bajt po bajcie\n",
    "                buf = f.read(1)\n",
    "                label = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "                result[i] = label\n",
    "            return result\n",
    "    except:\n",
    "        print(\"MNIST files not in directory/unable to read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(filepath: str) -> np.ndarray:\n",
    "    '''Metoda odczytująca  z pliku zgodnie ze specyfikacją ze strony MNIST'''\n",
    "    try:\n",
    "        f = open(filepath, 'rb')\n",
    "        # pierwsze cztery bajty to, znowu, magiczna liczba\n",
    "        f.read(4)\n",
    "        if f.mode == 'rb':\n",
    "            # kolejne zawierają liczbę zapisanych obrazów\n",
    "            buf = f.read(4)\n",
    "            number_of_images = int.from_bytes(buf,'big')\n",
    "            result = np.zeros(number_of_images, dtype ='object')\n",
    "            # ilość rzędów w pojedynczym obrazie\n",
    "            buf = f.read(4)\n",
    "            number_of_rows = int.from_bytes(buf,'big')\n",
    "            # ilość kolumn w pojedynczym obrazie\n",
    "            buf = f.read(4)\n",
    "            number_of_columns = int.from_bytes(buf,'big')\n",
    "            for i in range(0, number_of_images):\n",
    "                # ze względu na wygodę późniejszych obliczeń zdecydowałam się \n",
    "                # zapisać obrazy w postaci wektora o długości 28*28 i szerokości 1\n",
    "                result[i] = np.zeros([number_of_rows*number_of_columns], dtype='float')\n",
    "                for j in range(0, number_of_rows*number_of_columns):\n",
    "                    buf = f.read(1)\n",
    "                    # Ze względu na wygodę późniejszych obliczeń piksele są normalizowane\n",
    "                    # ze skali 0-256 do 0-1\n",
    "                    pixel = np.frombuffer(buf, dtype=np.uint8).astype(np.float64)/256\n",
    "                    if(pixel!=0): result[i][j] = pixel\n",
    "            return result\n",
    "    except:\n",
    "        print(\"MNIST files not in directory/unable to read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_elements(images: np.ndarray, labels: np.ndarray, elements: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "    '''Metoda usuwająca niepasujące wartości (tutaj 0 i 1)'''\n",
    "    to_delete = []\n",
    "    for i in range(0, len(labels)):\n",
    "        # Sprawdź, czy element należy do do usunięcia \n",
    "        # Jeżeli tak, dodaj indeks do listy\n",
    "        if(labels[i] in elements):\n",
    "            to_delete.append(i)\n",
    "    # Usunięcie niepotrzebnych elementów\n",
    "    images = np.delete(images, to_delete, axis = 0)\n",
    "    labels = np.delete(labels, to_delete)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytanie informacji z pliku\n",
    "images = np.concatenate((read_images(\"train-images.idx3-ubyte\"), read_images(\"t10k-images.idx3-ubyte\")))\n",
    "labels = np.concatenate((read_labels(\"train-labels.idx1-ubyte\"), read_labels(\"t10k-labels.idx1-ubyte\")))\n",
    "# Usunięcie obrazków oznaczonych wybranymi etykietami (tu 0 i 1)\n",
    "images, labels = delete_elements(images, labels, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przetasowanie zbiorów (aby uniknąć redundancji, używam metody przetasowywania z modelu)\n",
    "primes_train, primes_test, primes_label_train, primes_label_test = Model()._shuffle_datasets(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "training cost: -7394.00805\n",
      "validation loss: -1628.0080497061883\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6726.010683\n",
      "validation loss: -1597.010682783094\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6635.01217\n",
      "validation loss: -1578.0121699951098\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6598.013084\n",
      "validation loss: -1576.0130841821317\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6587.013678\n",
      "validation loss: -1577.0136776975824\n",
      "training index: 0\n",
      "validation index: 1\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6569.014077\n",
      "validation loss: -1569.0140772913921\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6561.014353\n",
      "validation loss: -1565.0143531188305\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6554.014547\n",
      "validation loss: -1562.0145469666643\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6557.014685\n",
      "validation loss: -1558.0146850784065\n",
      "training index: 1\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6548.014785\n",
      "validation loss: -1557.0147845661443\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6546.014857\n",
      "validation loss: -1557.0148568920902\n",
      "training index: 0\n",
      "validation index: 1\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6545.01491\n",
      "validation loss: -1557.0149098888617\n",
      "training index: 0\n",
      "validation index: 2\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6545.014949\n",
      "validation loss: -1556.0149489925132\n",
      "training index: 1\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6543.014978\n",
      "validation loss: -1556.0149780237225\n",
      "training index: 0\n",
      "validation index: 1\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6542.015\n",
      "validation loss: -1556.0149996966068\n",
      "training index: 0\n",
      "validation index: 2\n",
      "-----------------------------------\n",
      "epoch: 0\n",
      "training cost: -6541.015016\n",
      "validation loss: -1556.0150159571856\n",
      "training index: 0\n",
      "validation index: 3\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -7376.008667\n",
      "validation loss: -1661.0086668567633\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6687.011449\n",
      "validation loss: -1653.0114492297212\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6591.013007\n",
      "validation loss: -1639.0130068963986\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6530.013956\n",
      "validation loss: -1630.0139563800333\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6502.014569\n",
      "validation loss: -1633.0145686552182\n",
      "training index: 0\n",
      "validation index: 1\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6471.014979\n",
      "validation loss: -1637.0149787314272\n",
      "training index: 0\n",
      "validation index: 2\n",
      "-----------------------------------\n",
      "epoch: 1\n",
      "training cost: -6457.015261\n",
      "validation loss: -1634.0152605785718\n",
      "training index: 0\n",
      "validation index: 3\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -7439.008083\n",
      "validation loss: -1720.0080832783037\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6758.010697\n",
      "validation loss: -1682.0106970850954\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6646.012169\n",
      "validation loss: -1670.0121685777845\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6594.013074\n",
      "validation loss: -1665.0130740211146\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6574.013663\n",
      "validation loss: -1659.0136630277618\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6561.01406\n",
      "validation loss: -1655.0140602674871\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6545.014335\n",
      "validation loss: -1650.0143347766614\n",
      "training index: 0\n",
      "validation index: 0\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6543.014528\n",
      "validation loss: -1651.0145278115851\n",
      "training index: 0\n",
      "validation index: 1\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6540.014665\n",
      "validation loss: -1651.0146653785755\n",
      "training index: 0\n",
      "validation index: 2\n",
      "-----------------------------------\n",
      "epoch: 2\n",
      "training cost: -6540.014764\n",
      "validation loss: -1651.0147644837116\n",
      "training index: 1\n",
      "validation index: 3\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''Wyznaczenie odpowiednich parametrów. Aby uniknąć nadmiernego dopasowania modelu i \n",
    "eksplodujących gradientów, postanowiłam dodać do algorytmu regularyzację\n",
    "'''\n",
    "# Wartość kroku gradientu w SGD\n",
    "alpha = 5e-3\n",
    "# Wartość parametru beta, używanego w momentum\n",
    "beta = 0.99\n",
    "# Wartość parametru lambda, koniecznego do regularyzacji \n",
    "lamb = 0.9\n",
    "# Startowe wartości wag. Ponieważ używamy znormalizowanych \n",
    "# wartości pikseli, wygodnie jest zacząć od wag równych 0\n",
    "start_weights = np.zeros(len(primes_train[0]) + 1)\n",
    "\n",
    "primes_model = Model()\n",
    "primes_model.tune(start_weights, alpha = alpha, beta = beta, lamb = lamb)\n",
    "primes_model.fit(primes_train, primes_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predicting a prime digit in test set: 0.9154291923216226\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "test = []\n",
    "for image , label in zip(primes_test, primes_label_test):\n",
    "    pred.append(primes_model.predict(image))\n",
    "    # Przygotowanie etykietek do porównania\n",
    "    right_label = 1.0 if label in [2,3,5,7] else 0.0\n",
    "    test.append(right_label)\n",
    "primes_accuracy = primes_model.evaluate(np.array(pred), np.array(test))\n",
    "print(f'Accuracy of predicting a prime digit in test set: {primes_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Wnioski: \n",
    "- Podczas trenowania modelu osiagnięto skuteczność około 91.15%,\n",
    "- Próba skorzystania z modelu zbiorowego pozwoliła na podniesienie skuteczności do 92.21%, jednak znacznie wydłużyła czas trenowania i ciężar modelu,\n",
    "- Nasuwa się także pytanie dotyczące tego, jak mały błąd możemy osiagnąć w wyznaczonym problemie. Jeżeli nasz model zastosowany do wyznaczenia pojedynczej cyfry potrafi ją rozpoznać ze skutecznością x, czy skuteczność modelu tego samego typu rozpoznającego jedną z n cyfr możemy maksymalnie oszacować na x^n,\n",
    "- Dodanie regularyzacji okazało się korzystną decyzją. Wydłużyło czas trenowania modelu, ale pozwoliło na uniknięcie problemu eksplodujących wag (nagle pojawiających się wartości NaN).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bibliografia:\n",
    "- https://distill.pub/2017/momentum/\n",
    "- https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "- https://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Research_Engineer_Warszawa_i_Kraków",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
